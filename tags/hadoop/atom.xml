<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: hadoop | Blues 小站]]></title>
  <link href="http://www.shenyanchao.cn/tags/hadoop/atom.xml" rel="self"/>
  <link href="http://www.shenyanchao.cn/"/>
  <updated>2014-11-14T14:06:30+08:00</updated>
  <id>http://www.shenyanchao.cn/</id>
  <author>
    <name><![CDATA[ShenYanchao]]></name>
    <email><![CDATA[zhiyi.shen@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop1.2.1安装部署]]></title>
    <link href="http://www.shenyanchao.cn/blog/2014/11/13/install-hadoop/"/>
    <updated>2014-11-13T13:24:00+08:00</updated>
    <id>http://www.shenyanchao.cn/blog/2014/11/13/install-hadoop</id>
    <content type="html"><![CDATA[<h3>安装需求</h3>

<ul>
<li>Java 1.6</li>
<li>ssh,sshd正常安装</li>
</ul>


<p>确保可以ssh到localhost，并且不需要密码</p>

<pre><code>ssh localhost
</code></pre>

<p> 如果报错，connect to host localhost port 22:Connection refused。说明ssh-server未安装或者未启动。
 运行：</p>

<pre><code>ps -ef | grep sshd 
</code></pre>

<p>查看sshd进程是否存在，如果不存在，说明没有安装。那么进行安装。</p>

<pre><code>sudo apt-get install openssh-server
</code></pre>

<p>然后再执行<code>ssh localhost</code>,如果不能无密码登陆，需要做一下操作：</p>

<pre><code>    ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa 
    cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre>

<!--more-->


<h3>相关软件</h3>

<p>Ubuntu Linux为例：</p>

<pre><code> sudo apt-get install ssh 
 sudo apt-get install rsync
</code></pre>

<h3>Hadoop下载</h3>

<p>从<a href="http://hadoop.apache.org/releases.html">Hadoop官网</a>下载一个稳定版，这里就是1.2.1版本啦。</p>

<pre><code>wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz
</code></pre>

<p> ###启动Hadoop</p>

<p> 1.解压<code>tar -xzvf hadoop-1.2.1.tar.gz</code>，进入conf/hadoop-env.sh，设置JAVA_HOME为你的JDK目录。</p>

<pre><code>export JAVA_HOME=/path/to/java/home
</code></pre>

<p> 2.进入/hadoop-1.2.1目录，运行<code>bin\hadoop</code>,会显示hadoop的使用说明信息。</p>

<pre><code>  Usage: hadoop [--config confdir] COMMAND
  where COMMAND is one of:
  namenode -format     format the DFS filesystem
  secondarynamenode    run the DFS secondary namenode
  namenode             run the DFS namenode
  datanode             run a DFS datanode
  dfsadmin             run a DFS admin client
  mradmin              run a Map-Reduce admin client
  fsck                 run a DFS filesystem checking utility
  fs                   run a generic filesystem user client
  balancer             run a cluster balancing utility
  oiv                  apply the offline fsimage viewer to an fsimage
  fetchdt              fetch a delegation token from the NameNode
  jobtracker           run the MapReduce job Tracker node
  pipes                run a Pipes job
  tasktracker          run a MapReduce task Tracker node
  historyserver        run job history servers as a standalone daemon
  job                  manipulate MapReduce jobs
  queue                get information regarding JobQueues
  version              print the version
  jar &lt;jar&gt;            run a jar file
  distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively
  distcp2 &lt;srcurl&gt; &lt;desturl&gt; DistCp version 2
  archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive
  classpath            prints the class path needed to get the
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
 or
  CLASSNAME            run the class named CLASSNAME
Most commands print help when invoked w/o parameters.
</code></pre>

<p>  你可以下3中模式来启动hadoop：</p>

<ul>
<li>本地(standalone)模式</li>
<li>伪分布(Pseudo-Distributed)式</li>
<li>全分布(Full-Distributed)式</li>
</ul>


<h4>本地(Standalone)模式安装</h4>

<p>  默认情况下，Hadoop就是单机本地模式。方便调试。 <br/>
  下面是一个样例，复制conf目录到input作为输入，找出符合正则的文件，输出到output目录</p>

<pre><code>    $ mkdir input 
    $ cp conf/*.xml input 
    $ bin/hadoop jar hadoop-examples-*.jar grep input output 'dfs[a-z.]+' 
    $ cat output/*
</code></pre>

<h4>伪分布(Pseudo-Distributed)式</h4>

<p>配置conf/core-site.xml</p>

<pre><code>&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;fs.default.name&lt;/name&gt;
         &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>配置conf/hdfs-site.xml</p>

<pre><code>&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.replication&lt;/name&gt;
         &lt;value&gt;1&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>配置conf/mapred-site.xml:</p>

<pre><code>&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;mapred.job.tracker&lt;/name&gt;
         &lt;value&gt;localhost:9001&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>  格式化一个新的distributed-filesystem</p>

<pre><code> bin/hadoop namenode -format
</code></pre>

<p> 启动hadoop：</p>

<pre><code>bin/start-all.sh
</code></pre>

<p>  此时可以通过WEB UI控制台来监控NameNode，JobTracker，TaskTracker</p>

<ul>
<li>NameNode - <a href="http://localhost:50070/dfshealth.jsp/">http://localhost:50070/dfshealth.jsp/</a></li>
<li>JobTracker - <a href="http://localhost:50030/jobtracker.jsp/">http://localhost:50030/jobtracker.jsp/</a>
-TaskTracker - <a href="http://localhost:50060/tasktracker.jsp">http://localhost:50060/tasktracker.jsp</a></li>
</ul>


<p> 下面使用distributed filesystem来跑样例:
 拷贝conf目录到hadoop的input目录，此时可以在控制台看到创建了目录。</p>

<pre><code>$ bin/hadoop fs -put conf input
</code></pre>

<p>运行:</p>

<pre><code>$ bin/hadoop jar hadoop-examples-*.jar grep input output 'dfs[a-z.]+'
</code></pre>

<p>检查结果：</p>

<p>拷贝output目录到本地并检查:</p>

<pre><code>$ bin/hadoop fs -get output output 
$ cat output/*
</code></pre>

<p>或者直接在distributed filesystem查看:</p>

<pre><code>$ bin/hadoop fs -cat output/*
</code></pre>

<p>使用完，可以这样来关闭：</p>

<pre><code>$ bin/stop-all.sh
</code></pre>

<p> ###Hadoop分布式部署</p>

<h2> 详见<a href="http://hadoop.apache.org/docs/r1.2.1/cluster_setup.html">http://hadoop.apache.org/docs/r1.2.1/cluster_setup.html</a></h2>

<p>参考文档：<a href="http://hadoop.apache.org/docs/r1.2.1/single_node_setup.html">http://hadoop.apache.org/docs/r1.2.1/single_node_setup.html</a></p>
]]></content>
  </entry>
  
</feed>
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: encode | Blues 小站]]></title>
  <link href="http://www.shenyanchao.cn/tags/encode/atom.xml" rel="self"/>
  <link href="http://www.shenyanchao.cn/"/>
  <updated>2014-09-02T14:03:36+08:00</updated>
  <id>http://www.shenyanchao.cn/</id>
  <author>
    <name><![CDATA[ShenYanchao]]></name>
    <email><![CDATA[zhiyi.shen@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Java字符编码及其使用详解]]></title>
    <link href="http://www.shenyanchao.cn/blog/2012/10/23/encode-in-java/"/>
    <updated>2012-10-23T18:58:00+08:00</updated>
    <id>http://www.shenyanchao.cn/blog/2012/10/23/encode-in-java</id>
    <content type="html"><![CDATA[<h2>Java的编译存储</h2>

<p>Java是跨平台的一种语言，这一概念想必已经深入人心。Java是如何实现跨平台的呢?其中起到重大作用的便是Unicode编码。在使用IDE进行开发时，比如ECLIPSE,IDEA等，可以指定源文件（.java）的编码格式，此处的编码格式是指Java文件自身的编码。Java文件可以用各种编码进行存储，考虑到兼容中文字符，大多采用GBK,UTF-8,GB18030等编码格式。但是经过javac命令编译后，生成的.class文件毫无疑问都是Unicode编码。这样在class被加载进JVM后，所有的对象都是Unicode进行编码的，这确保了Java的跨平台特性。<br/>
<strong>简言之</strong></p>

<p>.java(任意编码) ---> .class(Unicode) ---> JVM内（Unicode）</p>

<!--more-->


<h2>是什么导致了乱码的出现？</h2>

<p>在JVM内，从class文件加载的源码全部以UNICODE编码。即使如<code>String str = "中国";</code>这样的语句，在JVM内存中仍然是unicode编码的。可是，程序本身难免牵涉到外部文件的读写、与数据库的交互等。这样就会造成很多非unicode编码的字符存在于JVM中，这也就是乱码出现的根本原因所在。</p>

<h2>Java中如何实现字符编码转换？</h2>

<p>String类提供了三个重要的函数：</p>

<pre><code>getBytes(String charsetName)
getBytes()
new String(byte[],String charsetName)
</code></pre>

<p><code>getBytes(String charset)</code>的作用是将字符串按照指定的charset编码，返回其字节方式的表示。具体来说，实现的是从unicode-->charset的转变。比如“中文”，在JVM内存储为“4e2d 6587”,如果设置charset为GBK,则被编码为“d6d0 cec4”。如果charset=UTF-8,那么结果是“e4 b8 ad e6 96 87”。如果charset=ISO-8859-1,由于无法编码，将返回“3f 3f”,这是两个问号。这是因为Unicode->ISO-8859-1不能完成这两个字符集之间的映射，因此使用时需要注意，这也为乱码提供了存在的可能性。</p>

<p><code>getBytes()</code>与上面这个功能一致，只不过charset是采用的系统默认的。系统默认的charset是什么呢？恐怕在不同的机器上有不同的charset。当前环境的默认charset可以通过<code>Charset.defaultCharset()</code>来查看，据个人测试，eclipse内是“UTF-8”,windows中文环境默认是“GBK”。因此当你写下getBytes()的时候，乱码的祸根已经种下，当程序在不同的环境运行时，结果可能就不一样，乱码就这样铺天盖地扑面而来了。哭吧！</p>

<p><code>new String(byte[],String charsetName)</code>的作用是将字节数组按照charset进行识别，最终转化为Unicode存储在JVM内。因此，如上面所说，如果byte[]是以UTF-8等编码存储的时候，如果按照ISO-8859-1这些不能映射的编码识别，仍旧出现乱码。</p>

<p>因此，使用诸如<code>new String(str.getBytes("utf-8"), "gbk")</code>这种类似的转化时，需要谨慎，防止字符集的不可映射造成的乱码。</p>

<p><strong>完美解决方案：</strong> <br/>
考虑到Java的编译存储，以及字符在JVM中的组织格式。其实需要自己完成的就是： <br/>
任意编码的字符 --> unicode --->任意编码的目标字符
也就是在InputStream时，指定源字符编码格式，这样Java会自动转换为JVM内部的Unicode格式。而在OutputStream时，指定目标编码格式，Java会自动从Unicode转化为目标编码。Unicode是一个桥梁，而这种转换是不需认为控制的。
以OutputStream为例：</p>

<pre><code>String str = "123中文";
System.out.println(str);
System.out.println("默认字符:" +Charset.defaultCharset());
BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(new File("e://encode.txt")),"GBK"));
writer.write(str);
writer.close();
</code></pre>

<p>在eclipse内，源文件是以UTF-8进行编码的，默认字符也为UTF-8,运行后encode.txt,内容正确显示，编码格式正确。</p>

<p>在windows cmd下进行javac,java进行执行。控制台输出“123 涓  .. ”等乱码。总之中文字符不能显示啊。查看encode.txt，内容倒是无乱码，编码格式怎么是UTF-8呢。我又一次纠结了？<code>System.out.println()</code>是不受环境影响的啊，它总能够以Unicode方式进行显示的啊。Java不至于这么弱吧。 <br/>
好吧，继续GOOGLE.BAIDU,找到了原因所在。javac 有这样一个参数<code>-encoding</code>来指定.java的编码格式。如果没有指定，那么编译时，认为编码格式为<code>Charset.defaultCharset()</code>,显然本例源码是以UTF-8进行编码的，直接javac是以GBK来读取的，那么在读到JVM成为UNICODE编码时，已经是乱码状态，不再是“123中文”了。<code>System.out.println()</code>只是按照实际情况进行了输出，因此在编译时，必须指定源文件的编码格式，保证了在Java文件内的常量字符的正常显示。</p>

<pre><code>javac -encoding utf-8 *.java    
</code></pre>

<p>这也就可以理解，maven中pom.xml定义编码的原因了：</p>

<pre><code>&lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
</code></pre>

<p>同样Ant的build.xml中，也有相应的编码配置选项：</p>

<pre><code>&lt;javac destdir="${build.dir}" encoding="UTF-8"&gt;
    &lt;src path="${src.dir}" /&gt;
    &lt;classpath refid="project.classpath" /&gt;
&lt;/javac&gt;
</code></pre>

<h2>HTTP的request与response乱码问题</h2>

<p>1.HTTP请求主要分为POST,GET两种情况。</p>

<blockquote><p>POST的情况下，如何设置数据的编码格式呢。首先可以通过<code>&lt;form accept-charset= "UTF-8"&gt;</code>来设置表单的编码格式。如果不设置，浏览器会直接使用网页的编码。JSP的网页编码一般通过<code>&lt;%@page pageEncoding="UTF-8"%&gt;</code>来指定JSP文件的<strong>存储编码</strong>。而通过<code>&lt;%@ page contentType="text/html; charset=UTF-8" %&gt;</code>来指定<strong>输出内容编码</strong>。<code>&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;</code>这一meta设置，用来指定<strong>网页编码</strong>，这一设置同样使用于静态页面。</p>

<p>GET情况下，参数都是放在URL内的，这时处理起来都比较麻烦。URL中的中文字符，一般都会进行<code>UrlEncode.encode()</code>处理,此时编码方式以系统默认编码为准，而在服务器端通过<code>getParameter</code>获得字符串是通过ISO-8859-1进行编码的。因此，需要从web服务器、浏览器端来同时考虑解决问题。服务器端的默认编码一般由<code>LC_ALL,LANG</code>决定的。通常可以设置为zh_CN.UTF-8。至于从浏览器端来解决，则没有统一的方法，毕竟环境多样。</p></blockquote>

<p>2.setCharacterEncoding()方法 <br/>
这一函数用来设置HTTP请求与相应的编码。前面提到过，通过<code>getParameter()</code>获得的字符串默认是以ISO-8859-1编码的。然而，如果使用了<code>request.setCharacterEncoding()</code>,则改变了其默认编码。同理，使用了<code>response.setCharacterEncoding</code>则保证了页面输出内容的编码，告诉浏览器输出内容采用什么编码。 <br/>
在spring的WEB项目中，有这样一个常用的filter：</p>

<pre><code>&lt;filter&gt;
    &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt;
    &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;
    &lt;init-param&gt;
        &lt;param-name&gt;encoding&lt;/param-name&gt;
        &lt;param-value&gt;UTF-8&lt;/param-value&gt;
    &lt;/init-param&gt;
    &lt;init-param&gt;
        &lt;param-name&gt;forceEncoding&lt;/param-name&gt;
        &lt;param-value&gt;true&lt;/param-value&gt;
    &lt;/init-param&gt;
&lt;/filter&gt;
</code></pre>

<p>这个filter，一看就知道是给编码有关，但是它具体做了哪些操作呢？源码来了：</p>

<pre><code>protected void doFilterInternal(
        HttpServletRequest request, HttpServletResponse response, FilterChain filterChain)
        throws ServletException, IOException {

    if (this.encoding != null &amp;&amp; (this.forceEncoding || request.getCharacterEncoding() == null)) {
        request.setCharacterEncoding(this.encoding);**
        if (this.forceEncoding) {
            response.setCharacterEncoding(this.encoding);
        }
    }
    filterChain.doFilter(request, response);
}
</code></pre>

<p>可以看出，就是做了request、response的setCharacterEncoding()操作，防止乱码的问题出现。</p>

<h2>数据库乱码</h2>

<p>前面讲到过，乱码的原因是JVM内可能会读到外界的未知编码字符，导致在内存中的Unicode编码为乱码。显然数据库就是其中之一，并且是经常遇到的问题。<br/>
1.首先，数据库要支持多语言，应该考虑将数据库设置成UTF或者Unicode编码，而UTF更适合存储（UTF是变长存储的）。如果中文数据中包含的英文字符很少，建议Unicode。数据库编码设置，一般通过在配置文件设置<code>default-character-set=utf8</code>，这个在建库的时间也可以直接指定。 <br/>
2.读库的时候，可以在JDBC连接串中指定读取编码。如<code>useUnicode=true&amp;characterEncoding=UTF-8</code>。并保证两者一致。</p>

<h2>SecureCRT类似工具乱码</h2>

<p>这种问题，通常是由于SecureCRT等客户端与LINUX环境编码不一致造成的。同样的，在这些客户端读取数据库内容进行显示出现乱码，也是由于编码不一致的问题。 <br/>
可以通过设置客户端的编码，以及设置Linux环境的编码，使其保持一致来解决。</p>

<p>参考文档：<a href="http://blog.csdn.net/qinysong/article/details/1179513">http://blog.csdn.net/qinysong/article/details/1179513</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[什么是BOM(Byte Order Mark)?]]></title>
    <link href="http://www.shenyanchao.cn/blog/2012/10/23/bom-in-utf8-text/"/>
    <updated>2012-10-23T17:05:00+08:00</updated>
    <id>http://www.shenyanchao.cn/blog/2012/10/23/bom-in-utf8-text</id>
    <content type="html"><![CDATA[<h3>引子</h3>

<p>在使用诸如UltraEditor,NotePad++等编辑工具时，经常会遇到encode进行转换的情况。以NotePad++为例，一篇ASCII编码的文本，通过菜单栏Encoding->convert to **进行编码格式的转换。我却发现一个奇怪的问题，下拉框里有这样两个选项：</p>

<ul>
<li>Convert to UTF-8</li>
<li>Convert to UTF-8 without BOM</li>
</ul>


<p>同样的是转化为UTF-8编码，为什么还牵涉到BOM呢。</p>

<!--more-->


<h3>原因</h3>

<p>1、big endian和little endian</p>

<p>big endian和little endian是CPU处理多字节数的不同方式。例如“汉”字的Unicode编码是6C49。那么写到文件里时，究竟是将6C写在前面，还是将49写在前面？如果将6C写在前面，就是big endian。还是将49写在前面，就是little endian。那么在读文件的时间 “endian”这个词出自《格列佛游记》。小人国的内战就源于吃鸡蛋时是究竟从大头(Big-Endian)敲开还是从小头(Little-Endian)敲开，由此曾发生过六次叛乱，其中一个皇帝送了命，另一个丢了王位。
我们一般将endian翻译成“字节序”，将big endian和little endian称作“大尾”和“小尾”。<br/>
UTF-8以字节为编码单元，没有字节序的问题。UTF-16以两个字节为编码单元，在解释一个UTF-16文本前，首先要弄清楚每个编码单元的字节序。例如收到一个“奎”的Unicode编码是594E，“乙”的Unicode编码是4E59。如果我们收到UTF-16字节流“594E”，那么这是“奎”还是“乙”？</p>

<p>2.如何解决以上问题？</p>

<p>在UCS 编码中有一个叫做"ZERO WIDTH NO-BREAK SPACE"的字符，它的编码是FEFF。而FFFE在UCS中是不存在的字符，所以不应该出现在实际传输中。UCS规范建议我们在传输字节流前，先传输字符"ZERO WIDTH NO-BREAK SPACE"。这样如果接收者收到FEFF，就表明这个字节流是Big-Endian的；如果收到FFFE，就表明这个字节流是Little-Endian的。因此字符"ZERO WIDTH NO-BREAK SPACE"又被称作BOM。</p>

<p>UTF-8不需要BOM来表明字节顺序，但可以用BOM来表明编码方式。字符"ZERO WIDTH NO-BREAK SPACE"的UTF-8编码是EF BB BF。所以如果接收者收到以EF BB BF开头的字节流，就知道这是UTF-8编码了。</p>

<p>3.BOM在XML中的使用</p>

<p> W3C定义了三条XML解析器如何正确读取XML文件的编码的规则：</p>

<ul>
<li>如果文档有BOM(字节顺序标记，一般来说，如果保存为unicode格式，则包含BOM，ANSI则无)，就定义了文件编码</li>
<li>如果没有BOM，就查看XML声明的编码属性</li>
<li>如果上述两个都没有，就假定XML文挡采用UTF-8编码</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[字符集编码]]></title>
    <link href="http://www.shenyanchao.cn/blog/2012/10/22/word-charset/"/>
    <updated>2012-10-22T20:29:00+08:00</updated>
    <id>http://www.shenyanchao.cn/blog/2012/10/22/word-charset</id>
    <content type="html"><![CDATA[<h2>GB2312字符集</h2>

<p>GB2312又称为GB2312-80字符集，全称为《信息交换用汉字编码字符集·基本集》，由原中国国家标准总局发布，1981年5月1日实施，是中国国家标准的简体中文字符集。它所收录的汉字已经覆盖99.75%的使用频率，基本满足了汉字的计算机处理需要。在中国大陆和新加坡获广泛使用。</p>

<!--more-->


<p>GB2312收录简化汉字及一般符号、序号、数字、拉丁字母、日文假名、希腊字母、俄文字母、汉语拼音符号、汉语注音字母，共 7445 个图形字符。其中包括6763个汉字，其中一级汉字3755个，二级汉字3008个；包括拉丁字母、希腊字母、日文平假名及片假名字母、俄语西里尔字母在内的682个全角字符。</p>

<p>GB2312中对所收汉字进行了“分区”处理，每区含有94个汉字/符号。这种表示方式也称为区位码。</p>

<p>它是用双字节表示的，两个字节中前面的字节为第一字节，后面的字节为第二字节。习惯上称第一字节为“高字节” ，而称第二字节为“低字节”。“高位字节”使用了0xA1-0xF7(把01-87区的区号加上0xA0)，“低位字节”使用了0xA1-0xFE(把01-94加上0xA0)。</p>

<p>以GB2312字符集的第一个汉字“啊”字为例，它的区号16，位号01，则区位码是1601，在大多数计算机程序中，高字节和低字节分别加0xA0得到程序的汉字处理编码0xB0A1。计算公式是：0xB0=0xA0+16, 0xA1=0xA0+1。</p>

<h2>GBK字符集</h2>

<p>GBK字符集是GB2312的扩展(K)，GBK1.0收录了21886个符号，它分为汉字区和图形符号区，汉字区包括21003个字符。GBK字符集主要扩展了繁体中文字的支持。</p>

<h2>BIG5 字符集</h2>

<p>BIG5又称大五码或五大码，1984年由台湾财团法人信息工业策进会和五间软件公司宏碁 (Acer)、神通 (MiTAC)、佳佳、零壹 (Zero One)、大众 (FIC)创立，故称大五码。Big5码的产生，是因为当时台湾不同厂商各自推出不同的编码，如倚天码、IBM PS55、王安码等，彼此不能兼容；另一方面，台湾政府当时尚未推出官方的汉字编码，而中国大陆的GB2312编码亦未有收录繁体中文字。</p>

<p>Big5字符集共收录13,053个中文字，该字符集在中国台湾使用。耐人寻味的是该字符集重复地收录了两个相同的字：“兀”(0xA461及0xC94A)、“嗀”(0xDCD1及0xDDFC)。</p>

<p>Big5码使用了双字节储存方法，以两个字节来编码一个字。第一个字节称为“高位字节”，第二个字节称为“低位字节”。高位字节的编码范围0xA1-0xF9，低位字节的编码范围0x40-0x7E及0xA1-0xFE。</p>

<p>尽管Big5码内包含一万多个字符，但是没有考虑社会上流通的人名、地名用字、方言用字、化学及生物科等用字，没有包含日文平假名及片假字母。</p>

<p>例如台湾视“着”为“著”的异体字，故没有收录“着”字。康熙字典中的一些部首用字(如“亠”、“疒”、“辵”、“癶”等)、常见的人名用字(如“堃”、“煊”、“栢”、“喆”等) 也没有收录到Big5之中。</p>

<h2>GB18030 字符集</h2>

<p>GB18030的全称是GB18030-2000《信息交换用汉字编码字符集基本集的扩充》，是我国政府于2000年3月17日发布的新的汉字编码国家标准，2001年8月31日后在中国市场上发布的软件必须符合本标准。GB 18030字符集标准的出台经过广泛参与和论证，来自国内外知名信息技术行业的公司，信息产业部和原国家质量技术监督局联合实施。</p>

<p>GB 18030字符集标准解决汉字、日文假名、朝鲜语和中国少数民族文字组成的大字符集计算机编码问题。该标准的字符总编码空间超过150万个编码位，收录了27484个汉字，覆盖中文、日文、朝鲜语和中国少数民族文字。满足中国大陆、香港、台湾、日本和韩国等东亚地区信息交换多文种、大字量、多用途、统一编码格式的要求。并且与Unicode 3.0版本兼容，填补Unicode扩展字符字汇“统一汉字扩展A”的内容。并且与以前的国家字符编码标准（GB2312，GB13000.1）兼容。</p>

<p>编码方法：<br/>
GB 18030标准采用单字节、双字节和四字节三种方式对字符编码。单字节部分使用0×00至0×7F码(对应于ASCII码的相应码)。双字节部分，首字节码从0×81至0×FE，尾字节码位分别是0×40至0×7E和0×80至0×FE。四字节部分采用GB/T 11383未采用的0×30到0×39作为对双字节编码扩充的后缀，这样扩充的四字节编码，其范围为0×81308130到0×FE39FE39。其中第一、三个字节编码码位均为0×81至0×FE，第二、四个字节编码码位均为0×30至0×39。</p>

<p>按照程序员的称呼，GB2312、GBK到GB18030都属于双字节字符集 (DBCS)。</p>

<p>接着是国际通用的unicode字符集</p>

<h2>Unicode字符集（简称为UCS）</h2>

<p>1．名称的由来</p>

<p>Unicode字符集编码是（Universal Multiple-Octet Coded Character Set） 通用多八位编码字符集的简称，支持世界上超过650种语言的国际字符集。Unicode允许在同一服务器上混合使用不同语言组的不同语言。它是由一个名为 Unicode 学术学会(Unicode Consortium)的机构制订的字符编码系统，支持现今世界各种不同语言的书面文本的交换、处理及显示。该编码于1990年开始研发，1994年正式公布，最新版本是2005年3月31日的Unicode 4.1.0。Unicode是一种在计算机上使用的字符编码。它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。</p>

<p>2．编码方法</p>

<p>Unicode 标准始终使用十六进制数字，而且在书写时在前面加上前缀“U+”，例如字母“A”的编码为 004116 。所以“A”的编码书写为“U+0041”。</p>

<p>3．UTF-8 编码 <br/>
UTF-8是Unicode的其中一个使用方式。 UTF是 Unicode Translation Format，即把Unicode转做某种格式的意思。</p>

<p>UTF-8便于不同的计算机之间使用网络传输不同语言和编码的文字，使得双字节的Unicode能够在现存的处理单字节的系统上正确传输。</p>

<p>UTF-8使用可变长度字节来储存 Unicode字符，例如ASCII字母继续使用1字节储存，重音文字、希腊字母或西里尔字母等使用2字节来储存，而常用的汉字就要使用3字节。辅助平面字符则使用4字节。</p>

<p>4．UTF-16 和 UTF-32 编码  <br/>
UTF-32、UTF-16 和 UTF-8 是 Unicode 标准的编码字符集的字符编码方案，UTF-16 使用一个或两个未分配的 16 位代码单元的序列对 Unicode 代码点进行编码；UTF-32 即将每一个 Unicode 代码点表示为相同值的 32 位整数</p>

<p>通过一个问题了解unicode编码</p>

<p><strong>问题：</strong>使用Windows记事本的“另存为”，可以在ANSI、GBK、Unicode、Unicode big endian和UTF-8这几种编码方式间相互转换。同样是txt文件，Windows怎样识别编码方式的呢？  <br/>
我很早前就发现Unicode、Unicode big endian和UTF-8编码的txt文件的开头会多出几个字节，分别是FF、FE（Unicode）,FE、FF（Unicode big endian）,EF、BB、BF（UTF-8）。但这些标记是基于什么标准呢？</p>

<p><strong>答案：</strong></p>

<p>ANSI字符集定义：ASCII字符集，以及由此派生并兼容的字符集，如：GB2312，正式的名称为MBCS(Multi-Byte Chactacter System，多字节字符系统)，通常也称为ANSI字符集。</p>

<h2>UNICODE 与 UTF8、UTF16</h2>

<p>由于每种语言都制定了自己的字符集，导致最后存在的各种字符集实在太多，在国际交流中要经常转换字符集非常不便。因此，产生了Unicode字符集，它固定使用16 bits(两个字节)来表示一个字符，共可以表示65536个字符  <br/>
标准的 Unicode 称为UTF-16(UTF:UCS Transformation Format )。后来为了双字节的Unicode能够在现存的处理单字节的系统上正确传输，出现了UTF-8，使用类似MBCS的方式对Unicode进行编码。(Unicode字符集有多种编码形式)  <br/>
例如"连通"两个字的Unicode标准编码UTF-16 (big endian)为：DE 8F 1A 90 <br/>
而其UTF-8编码为：E8 BF 9E E9 80 9A</p>

<p>当一个软件打开一个文本时，它要做的第一件事是决定这个文本究竟是使用哪种字符集的哪种编码保存的。软件一般采用三种方式来决定文本的字符集和编码：
检测文件头标识，提示用户选择，根据一定的规则猜测
最标准的途径是检测文本最开头的几个字节，开头字节 Charset/encoding,如下表：<br/>
EF BB BF UTF-8<br/>
FE FF UTF-16/UCS-2, little endian  <br/>
FF FE UTF-16/UCS-2, big endian  <br/>
FF FE 00 00 UTF-32/UCS-4, little endian. <br/>
00 00 FE FF UTF-32/UCS-4, big-endian.</p>

<p>1、big endian和little endian <br/>
big endian和little endian是CPU处理多字节数的不同方式。例如“汉”字的Unicode编码是6C49。那么写到文件里时，究竟是将6C写在前面，还是将49写在前面？如果将6C写在前面，就是big endian。还是将49写在前面，就是little endian。
“endian”这个词出自《格列佛游记》。小人国的内战就源于吃鸡蛋时是究竟从大头(Big-Endian)敲开还是从小头(Little-Endian)敲开，由此曾发生过六次叛乱，其中一个皇帝送了命，另一个丢了王位。 <br/>
我们一般将endian翻译成“字节序”，将big endian和little endian称作“大尾”和“小尾”。</p>

<p>2、字符编码、内码，顺带介绍汉字编码  <br/>
字符必须编码后才能被计算机处理。计算机使用的缺省编码方式就是计算机的内码。早期的计算机使用7位的ASCII编码，为了处理汉字，程序员设计了用于简体中文的GB2312和用于繁体中文的big5。  <br/>
GB2312(1980年)一共收录了7445个字符，包括6763个汉字和682个其它符号。汉字区的内码范围高字节从B0-F7，低字节从A1-FE，占用的码位是72<em>94=6768。其中有5个空位是D7FA-D7FE。 <br/>
GB2312支持的汉字太少。1995年的汉字扩展规范GBK1.0收录了21886个符号，它分为汉字区和图形符号区。汉字区包括21003个字符。2000年的GB18030是取代GBK1.0的正式国家标准。该标准收录了27484个汉字，同时还收录了藏文、蒙文、维吾尔文等主要的少数民族文字。现在的PC平台必须支持GB18030，对嵌入式产品暂不作要求。所以手机、MP3一般只支持GB2312。  <br/>
从ASCII、GB2312、GBK到GB18030，这些编码方法是向下兼容的，即同一个字符在这些方案中总是有相同的编码，后面的标准支持更多的字符。在这些编码中，英文和中文可以统一地处理。区分中文编码的方法是高字节的最高位不为0。按照程序员的称呼，GB2312、GBK到GB18030都属于双字节字符集 (DBCS)。  <br/>
有的中文Windows的缺省内码还是GBK，可以通过GB18030升级包升级到GB18030。不过GB18030相对GBK增加的字符，普通人是很难用到的，通常我们还是用GBK指代中文Windows内码。 <br/>
这里还有一些细节： <br/>
GB2312的原文还是区位码，从区位码到内码，需要在高字节和低字节上分别加上A0。 <br/>
在DBCS中，GB内码的存储格式始终是big endian，即高位在前。 <br/>
GB2312的两个字节的最高位都是1。但符合这个条件的码位只有128</em>128=16384个。所以GBK和GB18030的低字节最高位都可能不是1。不过这不影响DBCS字符流的解析：在读取DBCS字符流时，只要遇到高位为1的字节，就可以将下两个字节作为一个双字节编码，而不用管低字节的高位是什么。</p>

<p>3、Unicode、UCS和UTF(UCS Transformation Format)  <br/>
前面提到从ASCII、GB2312、GBK到GB18030的编码方法是向下兼容的。而Unicode只与ASCII兼容（更准确地说，是与ISO-8859-1兼容），与GB码不兼容。例如“汉”字的Unicode编码是6C49，而GB码是BABA。</p>

<p>UCS规定了怎么用多个字节表示各种文字。而怎样传输这些编码，是由UTF(UCS Transformation Format)规范规定的！常见的UTF规范包括UTF-8、UTF-7、UTF-16。</p>

<p>4、UTF的字节序和BOM <br/>
UTF-8以字节为编码单元，没有字节序的问题。UTF-16以两个字节为编码单元，在解释一个UTF-16文本前，首先要弄清楚每个编码单元的字节序。例如收到一个“奎”的Unicode编码是594E，“乙”的Unicode编码是4E59。如果我们收到UTF-16字节流“594E”，那么这是“奎”还是“乙”？ <br/>
Unicode规范中推荐的标记字节顺序的方法是BOM。BOM不是“Bill Of Material”的BOM表，而是Byte Order Mark。BOM是一个有点小聪明的想法：   <br/>
在UCS编码中有一个叫做"ZERO WIDTH NO-BREAK SPACE"的字符，它的编码是FEFF。而FFFE在UCS中是不存在的字符，所以不应该出现在实际传输中。UCS规范建议我们在传输字节流前，先传输字符"ZERO WIDTH NO-BREAK SPACE"。
这样如果接收者收到FEFF，就表明这个字节流是Big-Endian的；如果收到FFFE，就表明这个字节流是Little-Endian的。因此字符"ZERO WIDTH NO-BREAK SPACE"又被称作BOM。  <br/>
UTF-8不需要BOM来表明字节顺序，但可以用BOM来表明编码方式。字符"ZERO WIDTH NO-BREAK SPACE"的UTF-8编码是EF BB BF（读者可以用我们前面介绍的编码方法验证一下）。所以如果接收者收到以EF BB BF开头的字节流，就知道这是UTF-8编码了。  <br/>
Windows就是使用BOM来标记文本文件的编码方式的。</p>

<p>写到这里对编码有了大致的了解了，就可以理解网上一些文章的话了，比如有一篇很流行的文章《URL编码与SQL注射》里面有一段是这么说的：</p>

<p>其实url编码就是一个字符ascii码的十六进制。不过稍微有些变动，需要在前面加上“%”。比如“\”，它的ascii码是92，92的十六进制是5c，所以“\”的url编码就是%5c。那么汉字的url编码呢？很简单，看例子：“胡”的ascii码是-17670，十六进制是BAFA，url编码是“%BA%FA”。呵呵，知道怎么转换的了吧。</p>

<p>这得从ASCII说起，扩展的ASCII字符集采用8bit255个字符显然不够用，于是各个国家纷纷制定了自己的文字编码规范，其中中文的文字编码规范叫做“GB2312-80”（就是GB2312)，它是和ASCII兼容的一种编码规范，其实就是用扩展ASCII没有真正标准化这一点，把一个中文字符用两个扩展ASCII字符来表示。文中说的的中文ASCII码实际上就是简体中文的编码2312GB！它把ASCII又扩充了一个字节，由于高位的第一位是0，所以会出现负数的形式，url编码就是将汉字的这个GB2312编码转化成UTF-8的编码并且每8位即一个字节前面加上%符号表示。</p>

<p>那为何UTF-8是进行网络的规范传输编码呢？</p>

<p>在Unicode里，所有的字符被一视同仁。汉字不再使用“两个扩展ASCII”，而是使用“1个Unicode”，注意，现在的汉字是“一个字符”了，于是，拆字、统计字数这些问题也就自然而然的解决了。但是，这个世界不是理想的，不可能在一夜之间所有的系统都使用Unicode来处理字符，所以Unicode在诞生之日，就必须考虑一个严峻的问题：和ASCII字符集之间的不兼容问题。</p>

<p>我们知道，ASCII字符是单个字节的，比如“A”的ASCII是65。而Unicode是双字节的，比如“A”的Unicode是0065，这就造成了一个非常大的问题：以前处理ASCII的那套机制不能被用来处理Unicode了</p>

<p>另一个更加严重的问题是，C语言使用'\0'作为字符串结尾，而Unicode里恰恰有很多字符都有一个字节为0，这样一来，C语言的字符串函数将无法正常处理Unicode，除非把世界上所有用C写的程序以及他们所用的函数库全部换掉</p>

<p>于是，比Unicode更伟大的东东诞生了，之所以说它更伟大是因为它让Unicode不再存在于纸上，而是真实的存在于我们大家的电脑中。那就是：UTF</p>

<p>UTF= UCS Transformation Format UCS转换格式，它是将Unicode编码规则和计算机的实际编码对应起来的一个规则。现在流行的UTF有2种：UTF-8和UTF-16</p>

<p>其中UTF-16和上面提到的Unicode本身的编码规范是一致的，这里不多说了。而UTF-8不同，它定义了一种“区间规则”，这种规则可以和ASCII编码保持最大程度的兼容，这样做的好处是压缩了字符在西欧一些国家的内存消耗，减少了不必要的资源浪费，这在实际应用中是非常有必要的。</p>

<p>UTF-8有点类似于Haffman编码，它将Unicode编码为：<br/>
00000000-0000007F的字符，用单个字节来表示；</p>

<p>00000080-000007FF的字符用两个字节表示 （中文的编码范围）</p>

<p>00000800-0000FFFF的字符用3字节表示</p>

<p>因为目前为止Unicode-16规范没有指定FFFF以上的字符，所以UTF-8最多是使用3个字节来表示一个字符。但理论上来说，UTF-8最多需要用6字节表示一个字符。</p>

<p>在UTF-8里，英文字符仍然跟ASCII编码一样，因此原先的函数库可以继续使用。而中文的编码范围是在0080-07FF之间，因此是2个字节表示（但这两个字节和GB编码的两个字节是不同的）。</p>

<p>看看编码之多：ANSI,AscII,GB2312,GBK,BIG5,GB18030,Unicode,UCS（就是unicode）Utf-8,utf-16,utf-32 整整10种编码～，算是够复杂了
可是这还仅仅是个开始，应用方面变化无穷，不过现在看到这些东西起码再不会头大了！呼呼～</p>

<p>哦，漏了一个加密的base64编码。</p>

<h2>什么是Base64？</h2>

<p>按照RFC2045的定义，Base64被定义为：Base64内容传送编码被设计用来把任意序列的8位字节描述为一种不易被人直接识别的形式。（The Base64 Content-Transfer-Encoding is designed to represent arbitrary sequences of octets in a form that need not be humanly readable.）</p>

<p><strong>为什么要使用Base64？</strong></p>

<p>在设计这个编码的时候，我想设计人员最主要考虑了3个问题： <br/>
1.是否加密？  <br/>
2.加密算法复杂程度和效率  <br/>
3.如何处理传输？</p>

<p>加密是肯定的，但是加密的目的不是让用户发送非常安全的Email。这种加密方式主要就是“防君子不防小人”。即达到一眼望去完全看不出内容即可。 <br/>
基于这个目的加密算法的复杂程度和效率也就不能太大和太低。和上一个理由类似，MIME协议等用于发送Email的协议解决的是如何收发Email，而并不是如何安全的收发Email。因此算法的复杂程度要小，效率要高，否则因为发送Email而大量占用资源，路就有点走歪了。</p>

<p>但是，如果是基于以上两点，那么我们使用最简单的恺撒法即可，为什么Base64看起来要比恺撒法复杂呢？这是因为在Email的传送过程中，由于历史原因，Email只被允许传送ASCII字符，即一个8位字节的低7位。因此，如果您发送了一封带有非ASCII字符（即字节的最高位是1）的Email通过有“历史问题”的网关时就可能会出现问题。网关可能会把最高位置为0！很明显，问题就这样产生了！因此，为了能够正常的传送Email，这个问题就必须考虑！所以，单单靠改变字母的位置的恺撒之类的方案也就不行了。关于这一点可以参考RFC2046。  <br/>
基于以上的一些主要原因产生了Base64编码。</p>

<p>参考文档：<a href="http://cafardcn.iteye.com/blog/300239">http://cafardcn.iteye.com/blog/300239</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[字符集与编码的故事]]></title>
    <link href="http://www.shenyanchao.cn/blog/2012/10/22/encode-story/"/>
    <updated>2012-10-22T20:13:00+08:00</updated>
    <id>http://www.shenyanchao.cn/blog/2012/10/22/encode-story</id>
    <content type="html"><![CDATA[<p>很久很久以前，有一群人，他们决定用8个可以开合的晶体管来组合成不同的状态，以表示世界上的万物。他们看到8个开关状态是好的，于是他们把这称为"字节"。 再后来，他们又做了一些可以处理这些字节的机器，机器开动了，可以用字节来组合出很多状态，状态开始变来变去。他们看到这样是好的，于是它们就这机器称为"计算机"。</p>

<!--more-->


<p>开始计算机只在美国用。八位的字节一共可以组合出256(2的8次方)种不同的状态。 他们把其中的编号从0开始的32种状态分别规定了特殊的用途，一但终端、打印机遇上约定好的这些字节被传过来时，就要做一些约定的动作。遇上0x10, 终端就换行，遇上0x07, 终端就向人们嘟嘟叫，例好遇上0x1b, 打印机就打印反白的字，或者终端就用彩色显示字母。他们看到这样很好，于是就把这些0x20以下的字节状态称为"控制码"。</p>

<p>他们又把所有的空格、标点符号、数字、大小写字母分别用连续的字节状态表示，一直编到了第127号，这样计算机就可以用不同字节来存储英语的文字了。大家看到这样，都感觉很好，于是大家都把这个方案叫做 ANSI 的"ASCII"编码（American Standard Code for Information Interchange，美国信息互换标准代码）。当时世界上所有的计算机都用同样的ASCII方案来保存英文文字。</p>

<p>后来，就像建造巴比伦塔一样，世界各地的都开始使用计算机，但是很多国家用的不是英文，他们的字母里有许多是ASCII里没有的，为了可以在计算机保存他们的文字，他们决定采用127号之后的空位来表示这些新的字母、符号，还加入了很多画表格时需要用下到的横线、竖线、交叉等形状，一直把序号编到了最后一个状态255。从128到255这一页的字符集被称"扩展字符集"。从此之后，贪婪的人类再没有新的状态可以用了，美帝国主义可能没有想到还有第三世界国家的人们也希望可以用到计算机吧！</p>

<p>等中国人得到计算机时，已经没有可以利用的字节状态来表示汉字，况且有6000多个常用汉字需要保存呢。但是这难不倒智慧的中国人民，我们不客气地把那些127号之后的奇异符号们直接取消掉, 规定：一个小于127的字符的意义与原来相同，但两个大于127的字符连在一起时，就表示一个汉字，前面的一个字节（他称之为高字节）从0xA1用到0xF7，后面一个字节（低字节）从0xA1到0xFE，这样我们就可以组合出大约7000多个简体汉字了。在这些编码里，我们还把数学符号、罗马希腊的字母、日文的假名们都编进去了，连在 ASCII 里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的"全角"字符，而原来在127号以下的那些就叫"半角"字符了。  中国人民看到这样很不错，于是就把这种汉字方案叫做 "GB2312"。GB2312 是对 ASCII 的中文扩展。</p>

<p>但是中国的汉字太多了，我们很快就就发现有许多人的人名没有办法在这里打出来，特别是某些很会麻烦别人的国家领导人。于是我们不得不继续把 GB2312 没有用到的码位找出来老实不客气地用上。 后来还是不够用，于是干脆不再要求低字节一定是127号之后的内码，只要第一个字节是大于127就固定表示这是一个汉字的开始，不管后面跟的是不是扩展字符集里的内容。结果扩展之后的编码方案被称为 GBK 标准，GBK 包括了 GB2312 的所有内容，同时又增加了近20000个新的汉字（包括繁体字）和符号。  后来少数民族也要用电脑了，于是我们再扩展，又加了几千个新的少数民族的字，GBK 扩成了 GB18030。从此之后，中华民族的文化就可以在计算机时代中传承了。</p>

<p>中国的程序员们看到这一系列汉字编码的标准是好的，于是通称他们叫做 "DBCS"（Double Byte Charecter Set 双字节字符集）。在DBCS系列标准里，最大的特点是两字节长的汉字字符和一字节长的英文字符并存于同一套编码方案里，因此他们写的程序为了支持中文处理，必须要注意字串里的每一个字节的值，如果这个值是大于127的，那么就认为一个双字节字符集里的字符出现了。</p>

<p>因为当时各个国家都像中国这样搞出一套自己的编码标准，结果互相之间谁也不懂谁的编码，谁也不支持别人的编码，连大陆和台湾这样只相隔了150海里，使用着同一种语言的兄弟地区，也分别采用了不同的 DBCS 编码方案。当时的中国人想让电脑显示汉字，就必须装上一个"汉字系统"，专门用来处理汉字的显示、输入的问题，但是那个台湾的愚昧封建人士写的算命程序就必须加装另一套支持 BIG5 编码的什么"倚天汉字系统"才可以用，装错了字符系统，显示就会乱了套！这怎么办？而且世界民族之林中还有那些一时用不上电脑的穷苦人民，他们的文字又怎么办？ 真是计算机的巴比伦塔命题啊！</p>

<p>正在这时，大天使加百列及时出现了：一个叫 ISO （国际标谁化组织）的国际组织决定着手解决这个问题。他们采用的方法很简单：废了所有的地区性编码方案，重新搞一个包括了地球上所有文化、所有字母和符号的编码！他们打算叫它"Universal Multiple-Octet Coded Character Set"，简称 UCS, 俗称 "UNICODE"。</p>

<p>UNICODE 开始制订时，计算机的存储器容量极大地发展了，空间再也不成为问题了。于是 ISO 就直接规定必须用两个字节，也就是16位来统一表示所有的字符，对于ascii里的那些"半角"字符，UNICODE 包持其原编码不变，只是将其长度由原来的8位扩展为16位，而其他文化和语言的字符则全部重新统一编码。由于"半角"英文符号只需要用到低8位，所以其高8位永远是0，因此这种大气的方案在保存英文文本时会多浪费一倍的空间。</p>

<p>从前多种字符集存在时，那些做多语言软件的公司遇上过很大麻烦，他们为了在不同的国家销售同一套软件，就不得不在区域化软件时也加持那个双字节字符集咒语，不仅要处处小心不要搞错，还要把软件中的文字在不同的字符集中转来转去。UNICODE 对于他们来说是一个很好的一揽子解决方案，于是从 Windows NT 开始，MS 趁机把它们的操作系统改了一遍，把所有的核心代码都改成了用 UNICODE 方式工作的版本，从这时开始，WINDOWS 系统终于无需要加装各种本土语言系统，就可以显示全世界上所有文化的字符了。</p>

<p>但是，UNICODE 在制订时没有考虑与任何一种现有的编码方案保持兼容，这使得 GBK 与UNICODE 在汉字的内码编排上完全是不一样的，没有一种简单的算术方法可以把文本内容从UNICODE编码和另一种编码进行转换，这种转换必须通过查表来进行。</p>

<p>如前所述，UNICODE 是用两个字节来表示为一个字符，他总共可以组合出65535不同的字符，这大概已经可以覆盖世界上所有文化的符号。如果还不够也没有关系，ISO已经准备了UCS-4方案，说简单了就是四个字节来表示一个字符，这样我们就可以组合出21亿个不同的字符出来（最高位有其他用途），这大概可以用到银河联邦成立那一天吧！</p>

<p>UNICODE 来到时，一起到来的还有计算机网络的兴起，UNICODE 如何在网络上传输也是一个必须考虑的问题，于是面向传输的众多 UTF（UCS Transfer Format）标准出现了，顾名思义，UTF8就是每次8个位传输数据，而UTF16就是每次16个位，只不过为了传输时的可靠性，从UNICODE到UTF时并不是直接的对应，而是要过一些算法和规则来转换。</p>

<p>受到过网络编程加持的计算机僧侣们都知道，在网络里传递信息时有一个很重要的问题，就是对于数据高低位的解读方式，一些计算机是采用低位先发送的方法，例如我们PC机采用的 INTEL 架构，而另一些是采用高位先发送的方式，在网络中交换数据时，为了核对双方对于高低位的认识是否是一致的，采用了一种很简便的方法，就是在文本流的开始时向对方发送一个标志符。如果之后的文本是高位在位，那就发送"FEFF"，反之，则发送"FFFE"。不信你可以用二进制方式打开一个UTF-X格式的文件，看看开头两个字节是不是这两个字节？</p>

<p>讲到这里，我们再顺便说说一个很著名的奇怪现象：当你在 windows 的记事本里新建一个文件，输入"联通"两个字之后，保存，关闭，然后再次打开，你会发现这两个字已经消失了，代之的是几个乱码！呵呵，有人说这就是联通之所以拼不过移动的原因。</p>

<p>原文：<a href="http://turandot.iteye.com/blog/1703258">http://turandot.iteye.com/blog/1703258</a></p>
]]></content>
  </entry>
  
</feed>